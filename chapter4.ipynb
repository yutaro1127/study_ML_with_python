{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_path=os.path.join(mglearn.datasets.DATA_PATH,'adult.data')\n",
    "raw_data=pd.read_csv(\n",
    "    adult_path,\n",
    "    header=None,\n",
    "    index_col=False,\n",
    "    names=[\n",
    "        'age',\n",
    "        'workclass',\n",
    "        'fnlwgt',\n",
    "        'education',\n",
    "        'education_num',\n",
    "        'marital-status',\n",
    "        'occupation',\n",
    "        'relationship',\n",
    "        'race',\n",
    "        'gender',\n",
    "        'capital-gain',\n",
    "        'capital-loss',\n",
    "        'hours-per-week',\n",
    "        'native-country',\n",
    "        'income',\n",
    "    ]\n",
    ")\n",
    "data=raw_data[[\n",
    "        'age',\n",
    "        'workclass',\n",
    "        'education',\n",
    "        'gender',\n",
    "        'hours-per-week',\n",
    "        'occupation',\n",
    "        'income',\n",
    "]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dammies=pd.get_dummies(data)\n",
    "data_dammies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=data_dammies.loc[:,'age':'occupation_ Transport-moving']\n",
    "X=features.to_numpy()\n",
    "y=data_dammies['income_ >50K'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n",
    "\n",
    "scaler=StandardScaler().fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "\n",
    "lgr=LogisticRegression(max_iter=2000).fit(X_train_scaled,y_train)\n",
    "print(f'accurancy on train : {lgr.score(X_train_scaled,y_train):.2f}')\n",
    "print(f'accurancy on test : {lgr.score(X_test_scaled,y_test):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n",
    "\n",
    "lgr=LogisticRegression(max_iter=2000).fit(X_train,y_train)\n",
    "print(f'accurancy on train : {lgr.score(X_train,y_train):.2f}')\n",
    "print(f'accurancy on test : {lgr.score(X_test,y_test):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=mglearn.datasets.make_wave(n_samples=100)\n",
    "X=X.reshape(-1,1)\n",
    "line=np.linspace(-3,3,1000,endpoint=False).reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "reg=LinearRegression().fit(X,y)\n",
    "rfr=RandomForestRegressor(min_samples_split=3).fit(X,y)\n",
    "plt.scatter(X,y)\n",
    "plt.plot(line,reg.predict(line))\n",
    "plt.plot(line,rfr.predict(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "bins=np.linspace(-3,3,11)\n",
    "X_which_bin=np.digitize(X,bins)\n",
    "\n",
    "encoder=OneHotEncoder(sparse_output=False).fit(X_which_bin)\n",
    "X_binned=encoder.transform(X_which_bin)\n",
    "\n",
    "line_which_bin=np.digitize(line,bins)\n",
    "line_binned=encoder.transform(line_which_bin)\n",
    "\n",
    "reg=LinearRegression().fit(X_binned,y)\n",
    "rfr=RandomForestRegressor(min_samples_split=3).fit(X_binned,y)\n",
    "plt.vlines(bins,-3,3,linestyles='--',linewidth=1,color='gray',alpha=.2)\n",
    "plt.scatter(X,y)\n",
    "plt.plot(line,reg.predict(line_binned))\n",
    "plt.plot(line,rfr.predict(line_binned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined=np.hstack([X,X_binned])\n",
    "line_combined=np.hstack([line,line_binned])\n",
    "reg=LinearRegression().fit(X_combined,y)\n",
    "plt.vlines(bins,-3,3,linestyles='--',linewidth=1,color='gray',alpha=.2)\n",
    "plt.scatter(X,y)\n",
    "\n",
    "plt.plot(line,reg.predict(line_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_product=np.hstack([X_binned,X*X_binned])\n",
    "line_product=np.hstack([line_binned,line*line_binned])\n",
    "reg=LinearRegression().fit(X_product,y)\n",
    "plt.vlines(bins,-3,3,linestyles='--',linewidth=1,color='gray',alpha=.2)\n",
    "plt.scatter(X,y)\n",
    "\n",
    "plt.plot(line,reg.predict(line_product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly=PolynomialFeatures(degree=10,include_bias=False).fit(X)\n",
    "X_poly=poly.transform(X)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_poly=poly.transform(line)\n",
    "reg=LinearRegression().fit(X_poly,y)\n",
    "plt.scatter(X,y)\n",
    "\n",
    "plt.plot(line,reg.predict(line_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "plt.scatter(X,y,color='gray')\n",
    "plt.plot(line,reg.predict(line_poly),label='poly')\n",
    "\n",
    "for g in [1,5,10]:\n",
    "    svr=SVR(gamma=g).fit(X,y)\n",
    "    plt.plot(line,svr.predict(line),label=f'svr gamma={g}')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_h = fetch_california_housing()\n",
    "X_train,X_test,y_train,y_test=train_test_split(\n",
    "    cal_h.data,cal_h.target,random_state=0\n",
    ")\n",
    "scaler=MinMaxScaler().fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    cal_h.data,\n",
    "    columns=cal_h.feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly=PolynomialFeatures(degree=2).fit(X_train_scaled)\n",
    "X_train_poly=poly.transform(X_train_scaled)\n",
    "X_test_poly=poly.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge=Ridge().fit(X_train_scaled,y_train)\n",
    "print(f'accuracy on train without interaction :{ridge.score(X_train_scaled,y_train):.2f}')\n",
    "print(f'accuracy on test without interaction  :{ridge.score(X_test_scaled,y_test):.2f}')\n",
    "ridge=Ridge().fit(X_train_poly,y_train)\n",
    "print(f'accuracy on train with interaction    :{ridge.score(X_train_poly,y_train):.2f}')\n",
    "print(f'accuracy on test with interaction     :{ridge.score(X_test_poly,y_test):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestRegressor(n_estimators=100).fit(X_train_scaled,y_train)\n",
    "print(f'accuracy on train without interaction :{rf.score(X_train_scaled,y_train):.2f}')\n",
    "print(f'accuracy on test without interaction  :{rf.score(X_test_scaled,y_test):.2f}')\n",
    "rf=RandomForestRegressor(n_estimators=100).fit(X_train_poly,y_train)\n",
    "print(f'accuracy on train with interaction    :{rf.score(X_train_poly,y_train):.2f}')\n",
    "print(f'accuracy on test with interaction     :{rf.score(X_test_poly,y_test):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd=np.random.RandomState(0)\n",
    "X_org=rnd.normal(size=(1000,3))\n",
    "w=rnd.normal(size=3)\n",
    "X=rnd.poisson(10*np.exp(X_org))\n",
    "y=np.dot(X_org,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_0=np.bincount(X[:,0])\n",
    "bins_1=np.bincount(X[:,1])\n",
    "bins_2=np.bincount(X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes=plt.subplots(3,1,figsize=(15,10))\n",
    "for i,(ax,bins) in enumerate(zip(axes,[bins_0,bins_1,bins_2])):\n",
    "    ax.bar(x=range(len(bins)),height=bins)\n",
    "    ax.set_title(f'feature {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n",
    "\n",
    "ridge=Ridge().fit(X_train,y_train)\n",
    "print(f'accuracy on train without interaction :{ridge.score(X_train,y_train):.2f}')\n",
    "print(f'accuracy on test without interaction :{ridge.score(X_test,y_test):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_log=np.log(1+X_train)\n",
    "X_test_log=np.log(1+X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_train_log[:,0],bins=25,color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=Ridge().fit(X_train_log,y_train)\n",
    "print(f'accuracy on train without interaction :{ridge.score(X_train_log,y_train):.2f}')\n",
    "print(f'accuracy on test without interaction :{ridge.score(X_test_log,y_test):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectPercentile,f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer=load_breast_cancer()\n",
    "\n",
    "rng=np.random.RandomState(42)\n",
    "noise=rng.normal(size=(len(cancer.data),50))\n",
    "X_w_noise=np.hstack([cancer.data,noise])\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_w_noise,cancer.target,random_state=0,test_size=.5)\n",
    "\n",
    "select=SelectPercentile(score_func=f_classif,percentile=50,).fit(X_train,y_train)\n",
    "X_train_selected=select.transform(X_train)\n",
    "X_test_selected=select.transform(X_test)\n",
    "\n",
    "print(f'X train shape : {X_train.shape}')\n",
    "print(f'selected X train shape : {X_train_selected.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = select.get_support()\n",
    "print(mask)\n",
    "plt.matshow(mask.reshape(1,-1),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "m=3000\n",
    "logreg=LogisticRegression(max_iter=m).fit(X_train,y_train)\n",
    "# print(f'accuracy on train : {logreg.score(X_train,y_train):.2f}')\n",
    "print(f'accuracy on test : {logreg.score(X_test,y_test):.3f}')\n",
    "logreg=LogisticRegression(max_iter=m).fit(X_train_selected,y_train)\n",
    "# print(f'selected accuracy on train : {logreg.score(X_train_selected,y_train):.2f}')\n",
    "print(f'selected accuracy on test : {logreg.score(X_test_selected,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "select=SelectFromModel(\n",
    "    RandomForestRegressor(n_estimators=100,random_state=42),\n",
    "    threshold='median'\n",
    ")\n",
    "select.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_l1=select.transform(X_train)\n",
    "X_test_l1=select.transform(X_test)\n",
    "print(f'X train shape : {X_train.shape}')\n",
    "print(f'selected X train shape : {X_train_l1.shape}')\n",
    "\n",
    "mask = select.get_support()\n",
    "print(mask)\n",
    "plt.matshow(mask.reshape(1,-1),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=3000\n",
    "logreg=LogisticRegression(max_iter=m).fit(X_train,y_train)\n",
    "print(f'accuracy on test : {logreg.score(X_test,y_test):.5f}')\n",
    "logreg=LogisticRegression(max_iter=m).fit(X_train_selected,y_train)\n",
    "print(f'selected accuracy on test : {logreg.score(X_test_selected,y_test):.5f}')\n",
    "logreg=LogisticRegression(max_iter=m).fit(X_train_l1,y_train)\n",
    "print(f'l1 accuracy on test : {logreg.score(X_test_l1,y_test):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "select=RFE(\n",
    "    RandomForestRegressor(n_estimators=100,random_state=42),\n",
    "    n_features_to_select=40\n",
    ")\n",
    "select.fit(X_train,y_train)\n",
    "mask=select.get_support()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.reshape(1,-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(mask.reshape(1,-1),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe=select.transform(X_train)\n",
    "X_test_rfe=select.transform(X_test)\n",
    "print(f'X train shape : {X_train.shape}')\n",
    "print(f'selected X train shape : {X_train_rfe.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=3000\n",
    "logreg=LogisticRegression(max_iter=m).fit(X_train,y_train)\n",
    "print(f'accuracy on test : {logreg.score(X_test,y_test):.5f}')\n",
    "logreg=LogisticRegression(max_iter=m).fit(X_train_selected,y_train)\n",
    "print(f'selected accuracy on test : {logreg.score(X_test_selected,y_test):.5f}')\n",
    "logreg=LogisticRegression(max_iter=m).fit(X_train_l1,y_train)\n",
    "print(f'l1 accuracy on test : {logreg.score(X_test_l1,y_test):.5f}')\n",
    "logreg=LogisticRegression(max_iter=m).fit(X_train_rfe,y_train)\n",
    "print(f'rfe accuracy on test : {logreg.score(X_test_rfe,y_test):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citibike=mglearn.datasets.load_citibike()\n",
    "citibike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xticks=pd.date_range(start=citibike.index.min(),end=citibike.index.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.xticks(xticks,xticks.strftime('%a %m-%d'),rotation=90,ha='left')\n",
    "plt.plot(citibike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=citibike.values\n",
    "X=citibike.index.astype(int).to_numpy()\n",
    "X=X.reshape(-1,1)\n",
    "X=X//10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=len(citibike)\n",
    "train_size=23*8\n",
    "train_rate=train_size/sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_feature(features,targets,regressor):\n",
    "    X_train,X_test=features[:train_size],features[train_size:]\n",
    "    y_train,y_test=targets[:train_size],targets[train_size:]\n",
    "    # print(X_train.shape)\n",
    "    # print(y_train.shape)\n",
    "    regressor.fit(X_train,y_train)\n",
    "    print(f'Accurancy on train : {regressor.score(X_train,y_train):.2f}')\n",
    "    print(f'Accurancy on test : {regressor.score(X_test,y_test):.2f}')\n",
    "    plt.figure(figsize=(15,5))\n",
    "    # xticks=pd.date_range(start=np.min(features),end=np.max(features))\n",
    "    # print(xticks)\n",
    "    plt.xticks(\n",
    "        range(0,len(X),8),\n",
    "        xticks.strftime('%a %m-%d'),\n",
    "        rotation=90,\n",
    "        ha='left'\n",
    "    )\n",
    "    plt.plot(range(0,len(X_train)),y_train,label='train')\n",
    "    plt.plot(range(len(X_train),len(X)),y_test,label='test')\n",
    "    plt.plot(range(0,len(X_train)),regressor.predict(X_train),'--',label='predict_on_train')\n",
    "    plt.plot(range(len(X_train),len(X)),regressor.predict(X_test),'--',label='predict_on_test')\n",
    "    plt.legend(loc=(1.01,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 決定木ベースのランダムフォレストは外装性がない = 訓練データの変数の範囲内しか予測することができない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr=RandomForestRegressor(n_estimators=100,random_state=42)\n",
    "eval_on_feature(X,y,rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 変数を日付にすると、テスト変数が訓練変数の外側になってしまうので、現在と未来での共通の因子を変数に設定することでランダムフォレストでも未来の日付の予測を実装する  \n",
    "- まず具体的に、時刻による予測モデルを試す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour=citibike.index.hour.astype(int).to_numpy().reshape(-1,1)\n",
    "eval_on_feature(X_hour,y,rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 時刻を学習させることで未来予測の精度は上がったが、まだまだ\n",
    "- 時刻に加えて曜日情報を学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour_week=np.hstack([citibike.index.dayofweek.to_numpy().reshape(-1,1),X_hour])\n",
    "eval_on_feature(X_hour_week,y,rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ここまでランダムフォレストで実装したが、時刻と曜日による予測にランダムフォレストはコスパよくない。\n",
    "- このレベルであれば線型回帰で予測することができるので実装。\n",
    "- まずは何も考えずに"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "eval_on_feature(X_hour_week,y,LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 全然ダメダメ\n",
    "- 線型回帰は変数を線型(=比例)でしか学習しないので、曜日(0~6で入力されている)や時刻を連続値として線型に学習し、それぞれ単純な比例関係に解釈してしまっている。\n",
    "- 線型回帰でカテゴリ変数を使用する場合はone hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X_hour_week_encoded=OneHotEncoder().fit_transform(X_hour_week).toarray() # スパース行列での出力になるのでtoarray()\n",
    "eval_on_feature(X_hour_week_encoded,y,LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "eval_on_feature(X_hour_week_encoded.toarray(),y,Ridge())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 精度は上がってきた\n",
    "- 現状のモデルは時刻、曜日それぞれを独立に学習しているので、例えば日曜日の夕方、みたいな交互作用は学習していない\n",
    "- polynormal変数を作成して交互作用を学習させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly=PolynomialFeatures(\n",
    "    degree=2, # 曜日✖️時刻で２次元\n",
    "    interaction_only=True, # カテゴリ変数なので各変数の二乗は不要\n",
    "    include_bias=False\n",
    "    )\n",
    "X_hour_week_encoded_poly=poly.fit_transform(X_hour_week_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n",
    "eval_on_feature(X_hour_week_encoded_poly,y,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ランダムフォレストと同じくらいいい感じ！\n",
    "- 特長量エンジニアリングをして線型回帰するメリットは、どんな項目を学習することで精度が上がったかの解釈がしやすい（曜日✖️時刻の特長量に係数がつくから）\n",
    "- 係数を可視化する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特長量に名前をつける。\n",
    "# 最初の５つは月曜日始まりの曜日のone-hot\n",
    "# 続く８つは0時始まりの３時間ごとの時刻\n",
    "# そのあとは交互作用\n",
    "\n",
    "weekday = ['mon','tue','wed','thu','fri','sat','sun']\n",
    "hour = [f'{i:02}:00' for i in range(0,24,3)]\n",
    "features = weekday+hour\n",
    "features_poly = poly.get_feature_names_out(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_poly_nonzero=features_poly[lr.coef_ != 0]\n",
    "features_poly_nonzero=np.array(features_poly)[lr.coef_ != 0]\n",
    "coef_nonzero=lr.coef_[lr.coef_ != 0]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(coef_nonzero,'o')\n",
    "plt.xticks(range(len(coef_nonzero)),features_poly_nonzero,rotation=90)\n",
    "plt.xlim(-1,len(coef_nonzero)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_poly_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hour_week_encoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
